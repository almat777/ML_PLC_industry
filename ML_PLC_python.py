

#!/usr/bin/env python
# -*- coding: utf-8 -*-


# #  Iron Ore Flotation Process Optimization
# ## Machine Learning for Waste Minimization in Industrial Process
# 
# ---
# 
# ### ðŸ“‹ Project Description
# 
# **Objective:** Predict silica percentage (Silica %) in final concentrate to minimize waste and optimize iron ore production.
# 
# **Business Task:**
# - Waste minimization (% Silica) â†’ cost reduction
# - Flotation process parameter optimization
# - Quality control automation
# 
# 
# 
# **Dataset:** [Quality Prediction in a Mining Process (Kaggle)](https://www.kaggle.com/datasets/edumagalhaes/quality-prediction-in-a-mining-process)
# 


# ##  1. Import Libraries


# Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
import matplotlib.pyplot as plt
import seaborn as sns

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÑÑ‚Ð¸Ð»Ñ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¾Ð²
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")


# Machine Learning
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# XGBoost
import xgboost as xgb

# ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ
from scipy.optimize import minimize

print(" Ð’ÑÐµ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹!")
print(f"Pandas version: {pd.__version__}")
print(f"NumPy version: {np.__version__}")

# ##  2. Data Loading
# 
# **Instructions:**
# 1. Download dataset from Kaggle: https://www.kaggle.com/datasets/edumagalhaes/quality-prediction-in-a-mining-process
# 2. Unzip file `MiningProcess_Flotation_Plant_Database.csv`
# 3. Place it in the same folder as this notebook
# 
# Or use Kaggle API:
# ```bash
# kaggle datasets download -d edumagalhaes/quality-prediction-in-a-mining-process
# ```


# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°
df_full = pd.read_csv(r'C:\Users\Almat\Desktop\Machine_Learning\MiningProcess_Flotation_Plant_Database.csv', decimal=',')

print(f"Full dataset uploaded: {df_full.shape[0]:,} ÑÑ‚Ñ€Ð¾Ðº Ã— {df_full.shape[1]} ÑÑ‚Ð¾Ð»Ð±Ñ†Ð¾Ð²")
print(f"Memoty in storage: {df_full.memory_usage(deep=True).sum() / 1024**2:.2f} MB")



df_full.head()

# info about dataset
df_full.info()

# ## 3. Feature Selection and Dataset Simplification
# 
# For optimization, we select 9 key process parameters that have the most impact on silica content:
# 
# **1. Ore Quality (2 features):**
# - `% Iron Feed` - iron content in feed ore
# - `% Silica Feed` - silica content in feed ore
# 
# **2. Reagents (2 features):**
# - `Starch Flow` - starch dosage (depressant)
# - `Amina Flow` - amine dosage (collector)
# 
# **3. Process Conditions (3 features):**
# - `Ore Pulp Flow` - pulp flow rate
# - `Ore Pulp pH` - pulp pH level
# - `Ore Pulp Density` - pulp density
# 
# **4. Flotation Air (2 features):**
# - `Flotation Column 01 Air Flow` - air flow in column 1
# - `Flotation Column 02 Air Flow` - air flow in column 2
# 
# **Decision:** Keep `% Iron Feed` as it provides critical context for ore quality, even though it's not controllable.


# Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²
selected_features = [
    '% Iron Feed',
    '% Silica Feed',
    'Starch Flow',
    'Amina Flow',
    'Ore Pulp Flow',
    'Ore Pulp pH',
    'Ore Pulp Density',
    'Flotation Column 01 Air Flow',
    'Flotation Column 01 Level',
    'Flotation Column 02 Air Flow',
    '% Iron Concentrate',
    '% Silica Concentrate'  # TARGET
]

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑƒÐ¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ñ‹Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚
df = df_full[selected_features].copy()

print(f"Simplified dataset: {df.shape[0]:,} rows Ã— {df.shape[1]} columns")

# Ð¡ÑÐ¼Ð¿Ð»Ð¸Ñ€ÑƒÐµÐ¼ 100,000 ÑÑ‚Ñ€Ð¾Ðº Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ
np.random.seed(42)
df = df.sample(n=100000, random_state=42).reset_index(drop=True)

print(f"\n Final dataset: {df.shape[0]:,} ÑÑ‚Ñ€Ð¾Ðº Ã— {df.shape[1]} ÑÑ‚Ð¾Ð»Ð±Ñ†Ð¾Ð²")
print(f" Memory in storage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")


df.head(10)

# ##  4. Exploratory Data Analysis (EDA)
# 
# ### 4.1 Ð‘Ð°Ð·Ð¾Ð²Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°



df.describe()

# ### 4.2 ÐÐ½Ð°Ð»Ð¸Ð· Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð½Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹


# ==================== FIGURE 1: Missing values ====================

missing_data = pd.DataFrame({
    'Missing_Count': df.isnull().sum(),
    'Missing_Percent': (df.isnull().sum() / len(df)) * 100
}).sort_values('Missing_Percent', ascending=False)

print("\nðŸ“‹ Missing values by feature:")
print(missing_data[missing_data['Missing_Count'] > 0])

# Visualization of missing values
fig1 = plt.figure(figsize=(12, 6), num='Fig1_Missing_Values')
sns.heatmap(df.isnull(), cbar=True, cmap='viridis', yticklabels=False)
plt.title('Figure 1: Missing Values Heatmap', fontsize=14, fontweight='bold')
plt.xlabel('Features')
plt.tight_layout()

plt.show()      # Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¾ÐºÐ½Ð¾
plt.close(fig1) # Ð¿Ð¾ Ð¶ÐµÐ»Ð°Ð½Ð¸ÑŽ: Ð·Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ Ñ„Ð¸Ð³ÑƒÑ€Ñƒ Ð¿Ð¾ÑÐ»Ðµ Ð¿Ð¾ÐºÐ°Ð·Ð°

# ### 4.3 Correlation Analysis


# Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ ÐºÐ¾Ñ€Ñ€ÐµÐ»ÑÑ†Ð¸ÑŽ (Ð¸Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÑ NaN)
correlation_matrix = df.corr()

# Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
# ==================== FIGURE 2: Correlation Matrix ====================

fig2 = plt.figure(figsize=(14, 10), num='Fig2_Correlation_Matrix')

sns.heatmap(
    correlation_matrix,
    annot=True,
    fmt='.2f',
    cmap='coolwarm',
    center=0,
    square=True,
    linewidths=1,
    cbar_kws={"shrink": 0.8}
)

plt.title('Figure 2: Correlation Matrix', fontsize=16, fontweight='bold', pad=20)
plt.tight_layout()
plt.show()   # Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼, Ð±ÐµÐ· ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ


# ÐšÐ¾Ñ€Ñ€ÐµÐ»ÑÑ†Ð¸Ñ Ñ Ñ†ÐµÐ»ÐµÐ²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ (% Silica Concentrate)
target_correlation = correlation_matrix['% Silica Concentrate'].sort_values(ascending=False)

fig3 = plt.figure(figsize=(10, 6))
target_correlation[:-1].plot(kind='barh', color='steelblue')
plt.title('Correlation of features with % Silica Concentrate', fontsize=14, fontweight='bold')
plt.xlabel('Coefficient of correlation' )
plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()

print("\nÐšÐ¾Ñ€Ñ€ÐµÐ»ÑÑ†Ð¸Ñ Ñ Ñ†ÐµÐ»ÐµÐ²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ (% Silica Concentrate):")
print(target_correlation)

# ### 4.4 Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ñ†ÐµÐ»ÐµÐ²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹


# Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ % Silica Concentrate
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Ð“Ð¸ÑÑ‚Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð°
axes[0].hist(df['% Silica Concentrate'].dropna(), bins=50, color='coral', edgecolor='black', alpha=0.7)
axes[0].set_title('Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ % Silica Concentrate', fontsize=12, fontweight='bold')
axes[0].set_xlabel('% Silica Concentrate')
axes[0].set_ylabel('Ð§Ð°ÑÑ‚Ð¾Ñ‚Ð°')
axes[0].axvline(df['% Silica Concentrate'].mean(), color='red', linestyle='--', 
                linewidth=2, label=f'Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ: {df["% Silica Concentrate"].mean():.2f}')
axes[0].legend()
axes[0].grid(alpha=0.3)

# Box plot
axes[1].boxplot(df['% Silica Concentrate'].dropna(), vert=True, patch_artist=True,
                boxprops=dict(facecolor='lightblue', color='blue'),
                medianprops=dict(color='red', linewidth=2))
axes[1].set_title('Box Plot % Silica Concentrate', fontsize=12, fontweight='bold')
axes[1].set_ylabel('% Silica Concentrate')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(f"Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° % Silica Concentrate:")
print(f"  Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ: {df['% Silica Concentrate'].mean():.4f}")
print(f"  ÐœÐµÐ´Ð¸Ð°Ð½Ð°: {df['% Silica Concentrate'].median():.4f}")
print(f"  Ð¡Ñ‚Ð°Ð½Ð´. Ð¾Ñ‚ÐºÐ»Ð¾Ð½ÐµÐ½Ð¸Ðµ: {df['% Silica Concentrate'].std():.4f}")
print(f"  ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼: {df['% Silica Concentrate'].min():.4f}")
print(f"  ÐœÐ°ÐºÑÐ¸Ð¼ÑƒÐ¼: {df['% Silica Concentrate'].max():.4f}")

# ### 4.5 ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ðµ Ð²Ñ‹Ð±Ñ€Ð¾ÑÐ¾Ð² (Outliers)


# Box plots Ð´Ð»Ñ Ð²ÑÐµÑ… Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²
fig, axes = plt.subplots(3, 4, figsize=(16, 12))
axes = axes.ravel()

for idx, column in enumerate(df.columns):
    axes[idx].boxplot(df[column].dropna(), vert=True, patch_artist=True,
                     boxprops=dict(facecolor='lightgreen', alpha=0.7))
    axes[idx].set_title(column, fontsize=10)
    axes[idx].grid(alpha=0.3)

plt.suptitle('ÐÐ½Ð°Ð»Ð¸Ð· Ð²Ñ‹Ð±Ñ€Ð¾ÑÐ¾Ð² (Outliers Detection)', fontsize=16, fontweight='bold', y=1.00)
plt.tight_layout()
plt.show()

# ##  5. ÐŸÑ€ÐµÐ´Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… (Preprocessing)
# 
# ### 5.1 ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð½Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹


# Ð¡Ñ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ: ÑƒÐ´Ð°Ð»ÑÐµÐ¼ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ñ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸ÑÐ¼Ð¸ Ð² Ñ†ÐµÐ»ÐµÐ²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹
# ÐžÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ - Ð·Ð°Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð¼ÐµÐ´Ð¸Ð°Ð½Ð¾Ð¹

print(f"before: {df.shape}")

# Ð£Ð´Ð°Ð»ÑÐµÐ¼ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð³Ð´Ðµ target = NaN
df = df.dropna(subset=['% Silica Concentrate'])

# Ð—Ð°Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ¸ Ð¼ÐµÐ´Ð¸Ð°Ð½Ð¾Ð¹ Ð´Ð»Ñ Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº
for col in df.columns:
    if df[col].isnull().sum() > 0:
        median_value = df[col].median()
        df[col].fillna(median_value, inplace=True)
        print(f"  {col}: Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¾ {df[col].isnull().sum()} Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ¾Ð² Ð¼ÐµÐ´Ð¸Ð°Ð½Ð¾Ð¹ ({median_value:.2f})")

print(f"\nafter: {df.shape}")
print(f"missings: {df.isnull().sum().sum()}")

# ### 5.2 Feature Engineering
# 
# Ð¡Ð¾Ð·Ð´Ð°Ð´Ð¸Ð¼ Ð½Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð´Ð¾Ð¼ÐµÐ½Ð½Ñ‹Ñ… Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐµ Ñ„Ð»Ð¾Ñ‚Ð°Ñ†Ð¸Ð¸.


# 1. Ð¡Ð¾Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ðµ Iron/Silica Ð½Ð° Ð²Ñ…Ð¾Ð´Ðµ
df['Iron_Silica_Ratio'] = df['% Iron Feed'] / (df['% Silica Feed'] + 1e-6)  # +epsilon Ð´Ð»Ñ Ð¸Ð·Ð±ÐµÐ¶Ð°Ð½Ð¸Ñ Ð´ÐµÐ»ÐµÐ½Ð¸Ñ Ð½Ð° 0

# 2. Ð’Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ pH Ð¸ Density (Ð²Ð°Ð¶Ð½Ð¾ Ð´Ð»Ñ Ñ„Ð»Ð¾Ñ‚Ð°Ñ†Ð¸Ð¸)
df['pH_Density_Interaction'] = df['Ore Pulp pH'] * df['Ore Pulp Density']

# 3. Ð Ð°ÑÑ…Ð¾Ð´ Ñ€ÐµÐ°Ð³ÐµÐ½Ñ‚Ð° Ð½Ð° ÐµÐ´Ð¸Ð½Ð¸Ñ†Ñƒ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°
df['Starch_per_Flow'] = df['Starch Flow'] / (df['Ore Pulp Flow'] + 1e-6)

print(" Ð¡Ð¾Ð·Ð´Ð°Ð½Ñ‹ Ð½Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸:")
print("  - Iron_Silica_Ratio")
print("  - pH_Density_Interaction")
print("  - Starch_per_Flow")
print(f"\nÐ¢ÐµÐºÑƒÑ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²: {df.shape[1]}")

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÐ¾Ñ€Ñ€ÐµÐ»ÑÑ†Ð¸ÑŽ Ð½Ð¾Ð²Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² Ñ target
new_features = ['Iron_Silica_Ratio', 'pH_Density_Interaction', 'Starch_per_Flow']
new_corr = df[new_features + ['% Silica Concentrate']].corr()['% Silica Concentrate'].drop('% Silica Concentrate')

print("ÐšÐ¾Ñ€Ñ€ÐµÐ»ÑÑ†Ð¸Ñ Ð½Ð¾Ð²Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² Ñ Ñ†ÐµÐ»ÐµÐ²Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹:")
print(new_corr.sort_values(ascending=False))

# ### 5.3 Ð Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð½Ð° Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð¸ Ñ†ÐµÐ»ÐµÐ²ÑƒÑŽ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ


# Ð¦ÐµÐ»ÐµÐ²Ð°Ñ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ
target = '% Silica Concentrate'

# ÐŸÑ€Ð¸Ð·Ð½Ð°ÐºÐ¸ (Ð²ÑÐµ ÐºÑ€Ð¾Ð¼Ðµ target)
X = df.drop(columns=[target])
y = df[target]

print(f"Features (X): {X.shape}")
print(f"Target (y): {y.shape}")
print(f"\nÐŸÑ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ:")
for i, col in enumerate(X.columns, 1):
    print(f"  {i}. {col}")

# ### 5.4 Train-Test Split


# Ð Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð½Ð° train/test (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"Training set: {X_train.shape[0]:,} Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð²")
print(f"Test set: {X_test.shape[0]:,} Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð²")
print(f"\nÐ¡Ð¾Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ðµ: {X_train.shape[0]/len(X)*100:.1f}% train / {X_test.shape[0]/len(X)*100:.1f}% test")

# ### 5.5 ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²


# Ð¡Ñ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ (StandardScaler)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ð² DataFrame Ð´Ð»Ñ ÑƒÐ´Ð¾Ð±ÑÑ‚Ð²Ð°
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)

print(" ÐŸÑ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ StandardScaler")
print(f"\nÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… (Ð¿ÐµÑ€Ð²Ñ‹Ðµ 5 ÑÑ‚Ñ€Ð¾Ðº):")
X_train_scaled.head()

# ##  6. ÐœÐ¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ (Machine Learning)
# 
# ÐžÐ±ÑƒÑ‡Ð¸Ð¼ Ð¸ ÑÑ€Ð°Ð²Ð½Ð¸Ð¼ 4 Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¸:
# 1. **Linear Regression** (baseline)
# 2. **Ridge Regression** (Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ L2)
# 3. **Random Forest**
# 4. **XGBoost**
# 
# ### 6.1 Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹


def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):
    """
    ÐžÐ±ÑƒÑ‡Ð°ÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÑ‚ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°
    """
    # ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ
    model.fit(X_train, y_train)
    
    # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸
    metrics = {
        'Model': model_name,
        'Train RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),
        'Test RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),
        'Train MAE': mean_absolute_error(y_train, y_train_pred),
        'Test MAE': mean_absolute_error(y_test, y_test_pred),
        'Train RÂ²': r2_score(y_train, y_train_pred),
        'Test RÂ²': r2_score(y_test, y_test_pred)
    }
    
    return metrics, y_test_pred, model

print(" Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ evaluate_model ÑÐ¾Ð·Ð´Ð°Ð½Ð°")

# ### 6.2 ÐœÐ¾Ð´ÐµÐ»ÑŒ 1: Linear Regression (Baseline)



lr_model = LinearRegression()
lr_metrics, lr_pred, lr_trained = evaluate_model(
    lr_model, X_train_scaled, X_test_scaled, y_train, y_test, 'Linear Regression'
)

print(" Linear Regression Results:")
for key, value in lr_metrics.items():
    if key != 'Model':
        print(f"  {key}: {value:.4f}")

# ### 6.3 ÐœÐ¾Ð´ÐµÐ»ÑŒ 2: Ridge Regression



ridge_model = Ridge(alpha=1.0, random_state=42)
ridge_metrics, ridge_pred, ridge_trained = evaluate_model(
    ridge_model, X_train_scaled, X_test_scaled, y_train, y_test, 'Ridge Regression'
)

print(" Ridge Regression Results:")
for key, value in ridge_metrics.items():
    if key != 'Model':
        print(f"  {key}: {value:.4f}")

# ### 6.4 ÐœÐ¾Ð´ÐµÐ»ÑŒ 3: Random Forest



rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=20,
    min_samples_split=5,
    random_state=42,
    n_jobs=-1  # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð²ÑÐµ ÑÐ´Ñ€Ð° CPU
)

rf_metrics, rf_pred, rf_trained = evaluate_model(
    rf_model, X_train, X_test, y_train, y_test, 'Random Forest'
)

print(" Random Forest Results:")
for key, value in rf_metrics.items():
    if key != 'Model':
        print(f"  {key}: {value:.4f}")

# ### 6.5 ÐœÐ¾Ð´ÐµÐ»ÑŒ 4: XGBoost



xgb_model = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    n_jobs=-1
)

xgb_metrics, xgb_pred, xgb_trained = evaluate_model(
    xgb_model, X_train, X_test, y_train, y_test, 'XGBoost'
)

print(" XGBoost Results:")
for key, value in xgb_metrics.items():
    if key != 'Model':
        print(f"  {key}: {value:.4f}")

# ### 6.6 comparing models



results_df = pd.DataFrame([lr_metrics, ridge_metrics, rf_metrics, xgb_metrics])
results_df = results_df.set_index('Model')



print(results_df.to_string())


# Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ñ
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

metrics_to_plot = ['Test RMSE', 'Test MAE', 'Test RÂ²']
colors = ['#ff9999', '#66b3ff', '#99ff99']

for idx, metric in enumerate(metrics_to_plot):
    axes[idx].bar(results_df.index, results_df[metric], color=colors[idx], alpha=0.7, edgecolor='black')
    axes[idx].set_title(f'{metric}', fontsize=12, fontweight='bold')
    axes[idx].set_ylabel(metric)
    axes[idx].tick_params(axis='x', rotation=45)
    axes[idx].grid(axis='y', alpha=0.3)
    
    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð½Ð° ÑÑ‚Ð¾Ð»Ð±Ñ†Ñ‹
    for i, v in enumerate(results_df[metric]):
        axes[idx].text(i, v, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')

plt.suptitle('Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹', fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

# ### 6.7 Feature Importance (Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸)


# Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð»ÑƒÑ‡ÑˆÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¿Ð¾ Test RÂ²
best_model_name = results_df['Test RÂ²'].idxmax()
print(f"Ð›ÑƒÑ‡ÑˆÐ°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ: {best_model_name}")
print(f"   Test RÂ² = {results_df.loc[best_model_name, 'Test RÂ²']:.4f}")

# Ð”Ð»Ñ Random Forest Ð¸Ð»Ð¸ XGBoost Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ feature importance
if best_model_name == 'Random Forest':
    best_model = rf_trained
elif best_model_name == 'XGBoost':
    best_model = xgb_trained
else:
    best_model = None

if best_model is not None:
    # Feature importance
    feature_importance = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': best_model.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    # Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
    plt.figure(figsize=(12, 8))
    plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='teal', alpha=0.7)
    plt.xlabel('Importance', fontsize=12)
    plt.title(f'Feature Importance ({best_model_name})', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    print("\nÐ¢Ð¾Ð¿-10 Ð²Ð°Ð¶Ð½ÐµÐ¹ÑˆÐ¸Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²:")
    print(feature_importance.head(10).to_string(index=False))

# ### 6.8 Actual vs Predicted (Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸)


# ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸
if best_model_name == 'Random Forest':
    best_predictions = rf_pred
elif best_model_name == 'XGBoost':
    best_predictions = xgb_pred
elif best_model_name == 'Ridge Regression':
    best_predictions = ridge_pred
else:
    best_predictions = lr_pred

# Scatter plot: Actual vs Predicted
plt.figure(figsize=(10, 6))
plt.scatter(y_test, best_predictions, alpha=0.5, s=10)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
         'r--', lw=2, label='Ð˜Ð´ÐµÐ°Ð»ÑŒÐ½Ð°Ñ Ð»Ð¸Ð½Ð¸Ñ')
plt.xlabel('Ð¤Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ (Actual)', fontsize=12)
plt.ylabel('ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ (Predicted)', fontsize=12)
plt.title(f'Actual vs Predicted - {best_model_name}', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# ### 6.9 Residuals Analysis


# Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ð¾ÑÑ‚Ð°Ñ‚ÐºÐ¸ (residuals)
residuals = y_test - best_predictions

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Residuals plot
axes[0].scatter(best_predictions, residuals, alpha=0.5, s=10)
axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)
axes[0].set_xlabel('ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ', fontsize=11)
axes[0].set_ylabel('ÐžÑÑ‚Ð°Ñ‚ÐºÐ¸ (Residuals)', fontsize=11)
axes[0].set_title('Residuals Plot', fontsize=12, fontweight='bold')
axes[0].grid(alpha=0.3)

# Histogram of residuals
axes[1].hist(residuals, bins=50, color='skyblue', edgecolor='black', alpha=0.7)
axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)
axes[1].set_xlabel('ÐžÑÑ‚Ð°Ñ‚ÐºÐ¸ (Residuals)', fontsize=11)
axes[1].set_ylabel('Ð§Ð°ÑÑ‚Ð¾Ñ‚Ð°', fontsize=11)
axes[1].set_title('Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¾ÑÑ‚Ð°Ñ‚ÐºÐ¾Ð²', fontsize=12, fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(f"stat:")
print(f"  mean(avg): {residuals.mean():.6f}")
print(f"  Ð¡Ñ‚Ð°Ð½Ð´. Ð¾Ñ‚ÐºÐ»Ð¾Ð½ÐµÐ½Ð¸Ðµ: {residuals.std():.4f}")
print(f"  min: {residuals.min():.4f}")
print(f"  max: {residuals.max():.4f}")

# ##  7. ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°
# 
# Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð½Ð°Ñ…Ð¾Ð¶Ð´ÐµÐ½Ð¸Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¸Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÑŽÑ‚ % Silica Concentrate.
# 
# ### 7.1 ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ð¾Ð² Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²


# ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ boundaries Ð´Ð»Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…
# Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ min-max Ð¸Ð· Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…

# ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¾Ð¶Ð½Ð¾ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ (Ð¸ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ feed Ð¸ concentrate)
controllable_params = [
    'Starch Flow',
    'Amina Flow', 
    'Ore Pulp Flow',
    'Ore Pulp pH',
    'Ore Pulp Density',
    'Flotation Column 01 Air Flow',
    'Flotation Column 02 Air Flow'
]

# Ð“Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°
bounds = []
for param in controllable_params:
    bounds.append((X_train[param].min(), X_train[param].max()))

print("Ð”Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ñ‹ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð¸Ñ€ÑƒÐµÐ¼Ñ‹Ñ… Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²:")
for param, (min_val, max_val) in zip(controllable_params, bounds):
    print(f"  {param:30s}: [{min_val:8.2f}, {max_val:8.2f}]")

# ### 7.2 ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ scipy.optimize



def objective_function(controllable_values):
    """
    ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ % Silica Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð¸Ñ€ÑƒÐµÐ¼Ñ‹Ñ… Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð².
    ÐÐµÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð¸Ñ€ÑƒÐµÐ¼Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð±ÐµÑ€ÐµÐ¼ ÐºÐ°Ðº ÑÑ€ÐµÐ´Ð½Ð¸Ðµ Ð¸Ð· training set.
    """
   
    feature_values = {}
    

    for param, value in zip(controllable_params, controllable_values):
        feature_values[param] = value
   
    uncontrollable = ['% Iron Feed', '% Silica Feed', 'Flotation Column 01 Level', '% Iron Concentrate']
    for param in uncontrollable:
        feature_values[param] = X_train[param].mean()
    
    
    feature_values['Iron_Silica_Ratio'] = feature_values['% Iron Feed'] / (feature_values['% Silica Feed'] + 1e-6)
    feature_values['pH_Density_Interaction'] = feature_values['Ore Pulp pH'] * feature_values['Ore Pulp Density']
    feature_values['Starch_per_Flow'] = feature_values['Starch Flow'] / (feature_values['Ore Pulp Flow'] + 1e-6)
    
    
    X_pred = pd.DataFrame([feature_values])[X_train.columns]
    
   
    predicted_silica = best_model.predict(X_pred)[0]
    
    return predicted_silica




initial_params = [X_train[param].mean() for param in controllable_params]

print("ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ (ÑÑ€ÐµÐ´Ð½Ð¸Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ):")
for param, value in zip(controllable_params, initial_params):
    print(f"  {param:30s}: {value:10.2f}")

# ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²
initial_silica = objective_function(initial_params)
print(f"\n% Silica Ð´Ð»Ñ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²: {initial_silica:.4f}")

# ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð¯
print("Ð—Ð°Ð¿ÑƒÑÐº Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸...\n")

result = minimize(
    objective_function,
    initial_params,
    method='L-BFGS-B',
    bounds=bounds,
    options={'maxiter': 1000, 'disp': False}
)

if result.success:
    print(" ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°!\n")
    optimal_params = result.x
    optimal_silica = result.fun
else:
    print("ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð½Ðµ ÑÐ¾ÑˆÐ»Ð°ÑÑŒ")
    print(f"Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ: {result.message}")

# ### 7.3 Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸



optimization_results = pd.DataFrame({
    'ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€': controllable_params,
    'ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ': initial_params,
    'ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ': optimal_params,
    'Ð˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ': optimal_params - np.array(initial_params),
    'Ð˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ (%)': ((optimal_params - np.array(initial_params)) / np.array(initial_params)) * 100
})

print(optimization_results.to_string(index=False))
print("="*100)

print(f"\n Ð¦Ð•Ð›Ð•Ð’ÐÐ¯ ÐŸÐ•Ð Ð•ÐœÐ•ÐÐÐÐ¯ (% Silica Concentrate):")
print(f"   ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ: {initial_silica:.4f} %")
print(f"   ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ: {optimal_silica:.4f} %")
print(f"   Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ: {initial_silica - optimal_silica:.4f} % ({((initial_silica - optimal_silica)/initial_silica)*100:.2f}% ÑÐ½Ð¸Ð¶ÐµÐ½Ð¸Ðµ)")

print(f"\n Ð­ÐšÐžÐÐžÐœÐ˜Ð§Ð•Ð¡ÐšÐ˜Ð™ Ð­Ð¤Ð¤Ð•ÐšÐ¢ (Ð¿Ñ€Ð¸ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ 1000 Ñ‚/Ñ‡):")
waste_reduction = (initial_silica - optimal_silica) * 10  # Ñ‚Ð¾Ð½Ð½/Ñ‡Ð°Ñ
print(f"   Ð¡Ð½Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ñ…Ð¾Ð´Ð¾Ð²: {waste_reduction:.2f} Ñ‚Ð¾Ð½Ð½/Ñ‡Ð°Ñ")
print(f"   Ð­ÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ Ð² Ð¼ÐµÑÑÑ†: {waste_reduction * 24 * 30:.0f} Ñ‚Ð¾Ð½Ð½")
print(f"   Ð­ÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ Ð² Ð³Ð¾Ð´: {waste_reduction * 24 * 365:.0f} Ñ‚Ð¾Ð½Ð½")

print("\n" + "="*100)

# ### 7.4 Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸


# Bar chart ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ñ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²
fig, ax = plt.subplots(figsize=(14, 7))

x = np.arange(len(controllable_params))
width = 0.35

# ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð´Ð»Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ (0-1 scale)
initial_norm = [(initial_params[i] - bounds[i][0]) / (bounds[i][1] - bounds[i][0]) 
                for i in range(len(controllable_params))]
optimal_norm = [(optimal_params[i] - bounds[i][0]) / (bounds[i][1] - bounds[i][0]) 
                for i in range(len(controllable_params))]

ax.bar(x - width/2, initial_norm, width, label='ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ðµ', alpha=0.8, color='coral')
ax.bar(x + width/2, optimal_norm, width, label='ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ', alpha=0.8, color='lightgreen')

ax.set_xlabel('ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°', fontsize=12)
ax.set_ylabel('ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ (0-1)', fontsize=12)
ax.set_title('Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(controllable_params, rotation=45, ha='right')
ax.legend(fontsize=11)
ax.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

# ### 7.5 Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð´Ð»Ñ PLC



import json

plc_setpoints = {}
for param, value in zip(controllable_params, optimal_params):
    # Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð¼Ñ Ð´Ð»Ñ PLC (ÑƒÐ±Ð¸Ñ€Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹ Ð¸ ÑÐ¿ÐµÑ†.ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ‹)
    plc_tag = 'DB_Setpoints.' + param.replace(' ', '_').replace('%', 'Pct')
    plc_setpoints[plc_tag] = float(value)

# Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² JSON
with open('optimal_setpoints.json', 'w') as f:
    json.dump(plc_setpoints, f, indent=4)

print(" ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ setpoints ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð² 'optimal_setpoints.json'")
print("\nÐ¡Ð¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ Ñ„Ð°Ð¹Ð»Ð°:")
print(json.dumps(plc_setpoints, indent=4))

from opcua import Client

url = "opc.tcp://10.103.77.15:4840"
client = Client(url)

def walk(node, text, depth=0, max_depth=6, max_hits=30, hits=None):
    if hits is None:
        hits = []
    if depth > max_depth or len(hits) >= max_hits:
        return hits

    try:
        bn = node.get_browse_name()
        name = f"{bn.NamespaceIndex}:{bn.Name}"
    except:
        name = ""

    if text.lower() in name.lower():
        hits.append((name, node.nodeid.to_string()))

    try:
        for ch in node.get_children():
            walk(ch, text, depth + 1, max_depth, max_hits, hits)
    except:
        pass

    return hits

try:
    client.connect()
    print("Connected")

    ns = client.get_namespace_array()
    print("\nNamespace array:")
    for i, s in enumerate(ns):
        print(i, s)

    objects = client.get_objects_node()

    print("\nSearch for DB_Fall:")
    for name, nid in walk(objects, "DB_Fall", max_depth=8):
        print(name, "->", nid)

    print("\nSearch for Starch_Flow:")
    for name, nid in walk(objects, "Starch_Flow", max_depth=10):
        print(name, "->", nid)

finally:
    client.disconnect()
    print("Disconnected")
